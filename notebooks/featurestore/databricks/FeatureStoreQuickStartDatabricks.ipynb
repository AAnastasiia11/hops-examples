{"cells":[{"cell_type":"markdown","source":["# Feature Store Quick Start\n\nThis notebook gives you a quick overview of how you can intergrate the Feature Store on Hopsworks with Databricks and S3. We'll go over four steps:\n\n1. Generate some sample data and store it on S3\n2. Do some feature engineering with Databricks and the data from S3\n3. **Save the engineered features to the Feature Store**\n4. **Select a group of the features from the Feature Store and create a training dataset of tf records stored on S3**\n\n**This requires a Hopsworks instance that is available from Databricks, see [Databricks Quick Start](https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html).**"],"metadata":{}},{"cell_type":"markdown","source":["## Imports\n\nWe'll use numpy and pandas for data generation, pyspark for feature engineering, tensorflow and keras for model training, and the hops `featurestore` library for interacting with the feature store."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport random\nimport pandas as pd\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nfrom pyspark.sql import Row\nfrom hops import featurestore"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Setting up the Feature Store Connector"],"metadata":{}},{"cell_type":"code","source":["# setup_databricks only needs to be executed onces per cluster\n# Check the documentation for details: https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html#id8\nfeaturestore.setup_databricks(\n  'my-cluster.aws.hopsworks.ai',\n  'my-project',\n  region_name='my-region',\n  secrets_store='secretsmanager',\n  hostname_verification=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Connecting to the Feature Store"],"metadata":{}},{"cell_type":"code","source":["# Connect to the feature store, see https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html#step-4-3-connecting-to-the-feature-store\nfeaturestore.connect(\n  'my-cluster.aws.hopsworks.ai',\n  'my-project',\n  region_name='my-region',\n  secrets_store='secretsmanager',\n  hostname_verification=True)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Mounting an S3 bucket to Databricks"],"metadata":{}},{"cell_type":"code","source":["# Mount a bucket so that we can simulate a Datalake based on S3\n# This requires IAM roles to be set up for Databricks, see https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#access-s3-buckets-directly\nAWS_BUCKET_NAME = \"MY_S3_BUCKET\" # Ensure to replace with your bucket\nMOUNT_NAME = \"/mnt/demo_training_datasets\"\ndbutils.fs.mount(\"s3a://%s\" % AWS_BUCKET_NAME, MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Generate Sample Data\n\nLets generate two sample datasets and store them on S3:\n\n1. `houses_for_sale_data`:\n\n```bash\n+-------+--------+------------------+------------------+------------------+\n|area_id|house_id|       house_worth|         house_age|        house_size|\n+-------+--------+------------------+------------------+------------------+\n|      1|       0| 11678.15482418699|133.88670106643886|366.80067322738535|\n|      1|       1| 2290.436167500643|15994.969706808222|195.84014889823976|\n|      1|       2| 8380.774578431328|1994.8576926471007|1544.5164614303735|\n|      1|       3|11641.224696102923|23104.501275562343|1673.7222604337876|\n|      1|       4| 5382.089422436954| 13903.43637058141| 274.2912104765028|\n+-------+--------+------------------+------------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_id: long (nullable = true)\n |-- house_worth: double (nullable = true)\n |-- house_age: double (nullable = true)\n |-- house_size: double (nullable = true)\n```\n2. `houses_sold_data``\n```bash\n+-------+-----------------+-----------------+------------------+\n|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n+-------+-----------------+-----------------+------------------+\n|      1|                0|                0| 70073.06059070028|\n|      1|                1|               15| 146.9198329740602|\n|      1|                2|                6|  594.802165433149|\n|      1|                3|               10| 77187.84123130841|\n|      1|                4|                1|110627.48922722359|\n+-------+-----------------+-----------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_purchase_id: long (nullable = true)\n |-- number_of_bidders: long (nullable = true)\n |-- sold_for_amount: double (nullable = true)\n```\n\nWe'll use this data for predicting what a house is sold for based on features about the **area** where the house is."],"metadata":{}},{"cell_type":"markdown","source":["### Generation of `houses_for_sale_data`"],"metadata":{}},{"cell_type":"code","source":["area_ids = list(range(1,51))\nhouse_sizes = []\nhouse_worths = []\nhouse_ages = []\nhouse_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,100)):\n        house_sizes.append(abs(np.random.normal()*1000)/i)\n        house_worths.append(abs(np.random.normal()*10000)/i)\n        house_ages.append(abs(np.random.normal()*10000)/i)\n        house_area_ids.append(i)\nhouse_ids = list(range(len(house_area_ids)))\nhouses_for_sale_data  = pd.DataFrame({\n        'area_id':house_area_ids,\n        'house_id':house_ids,\n        'house_worth': house_worths,\n        'house_age': house_ages,\n        'house_size': house_sizes\n    })\nhouses_for_sale_data_spark_df = sqlContext.createDataFrame(houses_for_sale_data)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["houses_for_sale_data_spark_df.show(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["houses_for_sale_data_spark_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["houses_for_sale_data_spark_df.write.format(\"parquet\").save(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Generation of `houses_sold_data`"],"metadata":{}},{"cell_type":"code","source":["house_purchased_amounts = []\nhouse_purchases_bidders = []\nhouse_purchases_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,1000)):\n        house_purchased_amounts.append(abs(np.random.exponential()*100000)/i)\n        house_purchases_bidders.append(int(abs(np.random.exponential()*10)/i))\n        house_purchases_area_ids.append(i)\nhouse_purchase_ids = list(range(len(house_purchases_bidders)))\nhouses_sold_data  = pd.DataFrame({\n        'area_id':house_purchases_area_ids,\n        'house_purchase_id':house_purchase_ids,\n        'number_of_bidders': house_purchases_bidders,\n        'sold_for_amount': house_purchased_amounts\n    })\nhouses_sold_data_spark_df = sqlContext.createDataFrame(houses_sold_data)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["houses_sold_data_spark_df.show(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["houses_sold_data_spark_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["houses_sold_data_spark_df.write.format(\"parquet\").save(\"%s/houses_sold.parquet\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Feature Engineering\n\nLets generate some aggregate features such as sum and averages from our datasets on S3. \n\n1. `houses_for_sale_features`:\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_house_age: double (nullable = true)\n |-- avg_house_size: double (nullable = true)\n |-- avg_house_worth: double (nullable = true)\n |-- sum_house_age: double (nullable = true)\n |-- sum_house_size: double (nullable = true)\n |-- sum_house_worth: double (nullable = true)\n```\n\n2. `houses_sold_features`\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_num_bidders: double (nullable = true)\n |-- avg_sold_for: double (nullable = true)\n |-- sum_number_of_bidders: long (nullable = true)\n |-- sum_sold_for_amount: double (nullable = true)\n```"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Generate Features From `houses_for_sale_data`"],"metadata":{}},{"cell_type":"code","source":["houses_for_sale_data_spark_df = spark.read.parquet(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)\nsum_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_for_sale_df = sum_houses_for_sale_df.join(count_houses_for_sale_df, \"area_id\")\nsum_count_houses_for_sale_df = sum_count_houses_for_sale_df \\\n    .withColumnRenamed(\"sum(house_age)\", \"sum_house_age\") \\\n    .withColumnRenamed(\"sum(house_worth)\", \"sum_house_worth\") \\\n    .withColumnRenamed(\"sum(house_size)\", \"sum_house_size\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_house_for_sale(row):\n    avg_house_worth = row.sum_house_worth/float(row.num_rows)\n    avg_house_size = row.sum_house_size/float(row.num_rows)\n    avg_house_age = row.sum_house_age/float(row.num_rows)\n    return Row(\n        sum_house_worth=row.sum_house_worth, \n        sum_house_age=row.sum_house_age,\n        sum_house_size=row.sum_house_size,\n        area_id = row.area_id,\n        avg_house_worth = avg_house_worth,\n        avg_house_size = avg_house_size,\n        avg_house_age = avg_house_age\n       )\nhouses_for_sale_features_df = sum_count_houses_for_sale_df.rdd.map(\n    lambda row: compute_average_features_house_for_sale(row)\n).toDF()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["houses_for_sale_features_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Generate Features from `houses_sold_data`"],"metadata":{}},{"cell_type":"code","source":["houses_sold_data_spark_df = spark.read.parquet(\"%s/houses_sold.parquet\" % MOUNT_NAME)\nsum_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_sold_df = sum_houses_sold_df.join(count_houses_sold_df, \"area_id\")\nsum_count_houses_sold_df = sum_count_houses_sold_df \\\n    .withColumnRenamed(\"sum(number_of_bidders)\", \"sum_number_of_bidders\") \\\n    .withColumnRenamed(\"sum(sold_for_amount)\", \"sum_sold_for_amount\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_houses_sold(row):\n    avg_num_bidders = row.sum_number_of_bidders/float(row.num_rows)\n    avg_sold_for = row.sum_sold_for_amount/float(row.num_rows)\n    return Row(\n        sum_number_of_bidders=row.sum_number_of_bidders, \n        sum_sold_for_amount=row.sum_sold_for_amount,\n        area_id = row.area_id,\n        avg_num_bidders = avg_num_bidders,\n        avg_sold_for = avg_sold_for\n       )\nhouses_sold_features_df = sum_count_houses_sold_df.rdd.map(\n    lambda row: compute_average_features_houses_sold(row)\n).toDF()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["houses_sold_features_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Save Features to the Feature Store\n\nThe Featue store has an abstraction of a **feature group** which is a set of features that naturally belong together that typically are computed using the same feature engineering job and the same raw dataset. \n\nLets create two feature groups:\n\n1. `houses_for_sale_featuregroup`\n\n2. `houses_sold_featuregroup`"],"metadata":{}},{"cell_type":"code","source":["featurestore.create_featuregroup(\n    houses_for_sale_features_df,\n    \"houses_for_sale_featuregroup\",\n    description=\"aggregate features of houses for sale per area\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["featurestore.create_featuregroup(\n    houses_sold_features_df,\n    \"houses_sold_featuregroup\",\n    description=\"aggregate features of sold houses per area\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Create a Training Dataset\n\nThe feature store has an abstraction of a **training dataset**, which is a dataset with a set of features (potentially from many different feature groups) and labels (in case of supervised learning). \n\nLet's create a training dataset called *predict_house_sold_for_dataset* using the following features:\n\n- avg_house_age\n- avg_house_size\n- avg_house_worth\n- avg_num_bidders\n\nand the target variable is:\n\n- avg_sold_for"],"metadata":{}},{"cell_type":"code","source":["features_df = featurestore.get_features([\"avg_house_age\", \"avg_house_size\", \n                                         \"avg_house_worth\", \"avg_num_bidders\", \n                                         \"avg_sold_for\"])\n\nfeaturestore.create_training_dataset(\n    features_df, \"predict_house_sold_for_dataset_two\",\n    data_format=\"tfrecords\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False,\n    sink='MY_FEATURE_STORE_CONNECTOR_FOR_S3' # See https://hopsworks.readthedocs.io/en/latest/featurestore/guides/featurestore.html#configuring-storage-connectors-for-the-feature-store\n)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(dbutils.fs.ls(MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":2},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"ipython3"},"name":"FeatureStoreQuickStart","notebookId":2370176114424714},"nbformat":4,"nbformat_minor":0}
